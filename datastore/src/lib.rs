// Hide warnings generated by Plex
#![allow(
    clippy::redundant_closure_call,
    clippy::unused_unit,
    clippy::match_single_binding,
    clippy::let_unit_value,
    clippy::unit_arg,
    clippy::ptr_arg,
    clippy::comparison_chain,
    clippy::unnested_or_patterns,
    unused_braces
)]

use sha2::Digest as HashDigest;
use std::collections::{BTreeMap, HashMap, HashSet};
use std::sync::{Arc, Mutex, RwLock, RwLockWriteGuard};

use blockchain_simulator::AccountId;

#[macro_use]
extern crate static_assertions;

use sha2::Sha512;

mod reservations;

pub mod merkle;
pub mod patricia;

mod reservation_set;
pub use reservation_set::ReservationSet;

mod meta_fn_parser;
pub use meta_fn_parser::{MetaFnDefinition, MetaOperation};

mod application_parser;
pub use application_parser::*;

mod types;
pub use types::TypeRegistry;

pub mod values;
use values::Value;

pub mod objects;
use objects::*;

use merkle::FrozenMerkleTree;
use patricia::FrozenPatriciaTree;

pub mod protocol;
use protocol::{TransactionFlag, TxId};

mod shards;
use shards::DataShard;

mod error;
pub use error::DatastoreError;

mod digest;
pub use digest::{
    BatchStatistics, Digest, DigestAppendResult, DigestBatch, DigestStatistics, PendingDigestBatch,
};

mod transactions;
pub use transactions::*;

pub const DEFAULT_ENCLAVE_PORT: u16 = 18080;
pub const DEFAULT_PROXY_PORT: u16 = 5080;

pub type ShardId = u32;

pub struct Datastore {
    shards: Vec<RwLock<DataShard>>,
    type_manager: Arc<TypeRegistry>,
    pending_digest: Mutex<PendingDigestBatch>,
    digest: Digest,

    // meta mutex we use to ensure there are no deadlocks when locking shards
    meta_mutex: Mutex<()>,
}

pub struct EpochHashes {
    pub state_tree: FrozenPatriciaTree,
    pub transaction_tree: FrozenMerkleTree,
    pub reservation_tree: FrozenMerkleTree,
}

pub type ShardLockMap<'a> = HashMap<ShardId, RwLockWriteGuard<'a, DataShard>>;

impl Default for Datastore {
    fn default() -> Self {
        // Need more than one shard and shards need to be a power of two
        const_assert!(Datastore::NUM_SHARDS > 1);
        const_assert!((Datastore::NUM_SHARDS & (Datastore::NUM_SHARDS - 1)) == 0);

        let type_manager = Arc::new(TypeRegistry::default());

        let mut shards = Vec::new();

        for _ in 0..Datastore::NUM_SHARDS {
            let shard = DataShard::new(type_manager.clone());
            shards.push(RwLock::new(shard));
        }

        let pending_digest = Mutex::new(PendingDigestBatch::default());

        let digest = Digest::default();
        let meta_mutex = Mutex::new(());

        Self {
            shards,
            meta_mutex,
            pending_digest,
            digest,
            type_manager,
        }
    }
}

impl Datastore {
    pub const NUM_SHARDS: usize = patricia::CHILDREN_PER_BIGNODE;

    pub fn append_lock(&self, uid: ObjectUid, path: &[String], owner: TxId) -> bool {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.append_lock(uid, path, owner)
    }

    pub fn append_unlock(&self, uid: ObjectUid, path: &[String], owner: TxId) {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.append_unlock(uid, path, owner);
    }

    pub fn write_lock(&self, uid: ObjectUid, path: &[String], owner: TxId) -> bool {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.write_lock(uid, path, owner)
    }

    pub fn write_unlock(&self, uid: ObjectUid, path: &[String], owner: TxId) {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.write_unlock(uid, path, owner)
    }

    pub fn read_lock(&self, uid: ObjectUid, path: &[String], owner: TxId) -> bool {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.read_lock(uid, path, owner)
    }

    pub fn read_unlock(&self, uid: ObjectUid, path: &[String], owner: TxId) {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.read_unlock(uid, path, owner)
    }

    // Lock a set of shards and return handles to them
    // (needed for transaction processing)
    pub fn get_shards(&self, ids: HashSet<ShardId>) -> ShardLockMap {
        let mut locks = HashMap::new();
        let _tx_lock = self.meta_mutex.lock();

        for sid in ids {
            let l = self.shards[sid as usize].write().unwrap();
            locks.insert(sid, l);
        }

        locks
    }

    pub fn is_digest_full(&self) -> bool {
        let digest_batch = self.pending_digest.lock().unwrap();
        digest_batch.is_full()
    }

    // This doesn't actually commit a transaction but marks it to be included into the next batch
    pub fn record_transaction(
        &self,
        source: AccountId,
        tx_id: TxId,
        flag: TransactionFlag,
    ) -> DigestAppendResult {
        let mut digest_batch = self.pending_digest.lock().unwrap();
        digest_batch.append_entry(source, tx_id, flag)
    }

    pub fn update_hashes(&self) -> Option<EpochHashes> {
        // Keep pending digest locked throughout the function
        // To ensure atomicity
        let mut pending = self.pending_digest.lock().unwrap();
        let mut tx_hashes = Vec::new();

        let batch = if pending.is_empty() {
            // no change
            return None;
        } else {
            let batch = std::mem::take(&mut *pending);
            for e in batch.entries.iter() {
                let mut hasher = Sha512::new();
                let bytes = bincode::serialize(&e).unwrap();
                hasher.update(&bytes);

                tx_hashes.push(hasher.finalize());
            }

            batch
        };

        // Lock everything!
        // We want an atomic snapshot of the state and not see partial tx commits.
        let mut locks = Vec::new();
        {
            let _tx_lock = self.meta_mutex.lock();

            for shard in &self.shards {
                locks.push(shard.write().unwrap());
            }
        }

        let mut shard_strees = Vec::new();
        let mut shard_rtrees = Vec::new();

        //TODO actually use more than one thread here?
        for lock in &mut locks {
            let shard = &mut *lock;
            shard.update_hashes();

            shard_strees.push(lock.get_state_hashes());
            shard_rtrees.push(lock.get_reservation_hashes());
        }

        let state_tree = FrozenPatriciaTree::new_from_children(shard_strees);
        let transaction_tree = FrozenMerkleTree::new_from_hashes(tx_hashes);

        let reservation_tree = FrozenMerkleTree::new_from_children(shard_rtrees);

        let batch = batch.seal(
            *transaction_tree.get_hash(),
            *state_tree.get_hash(),
            *reservation_tree.get_hash(),
        );
        self.digest.append_batch(batch);

        Some(EpochHashes {
            transaction_tree,
            state_tree,
            reservation_tree,
        })
    }

    #[inline]
    pub fn uid_to_shard(uid: ObjectUid) -> ShardId {
        (uid % Datastore::NUM_SHARDS as u64) as ShardId
    }

    pub fn get(&self, uid: ObjectUid) -> Result<Value, DatastoreError> {
        let fields = [""; 0];
        self.get_field(uid, &fields)
    }

    pub fn get_field<T: AsRef<str>>(
        &self,
        uid: ObjectUid,
        path: &[T],
    ) -> Result<Value, DatastoreError> {
        let sid = Datastore::uid_to_shard(uid);
        let shard = self.shards[sid as usize].read().unwrap();

        shard.get_field(uid, path)
    }

    pub fn list_append<T: AsRef<str> + ToString>(
        &self,
        uid: ObjectUid,
        path: &[T],
        value: Value,
    ) -> Result<(), DatastoreError> {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.list_append(uid, path, value)
    }

    pub fn map_insert<T: AsRef<str> + ToString>(
        &self,
        uid: ObjectUid,
        path: &[T],
        value: Value,
    ) -> Result<(), DatastoreError> {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.map_insert(uid, path, value)
    }

    pub fn get_owner(&self, uid: ObjectUid) -> Result<AccountId, DatastoreError> {
        let sid = Datastore::uid_to_shard(uid);
        let shard = self.shards[sid as usize].write().unwrap();

        shard.get_owner(uid)
    }

    pub fn create(
        &self,
        uid: ObjectUid,
        owner: AccountId,
        application: &str,
        typeid: ObjectTypeId,
        content: HashMap<String, Value>,
    ) -> Result<(), DatastoreError> {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.create(uid, owner, application, typeid, content)
    }

    pub fn update(&self, uid: ObjectUid, value: Value) -> Result<(), DatastoreError> {
        let sid = Datastore::uid_to_shard(uid);
        let mut shard = self.shards[sid as usize].write().unwrap();

        shard.update(uid, value)
    }

    pub fn register_application(
        &self,
        name: String,
        types: BTreeMap<String, (u16, ObjectFieldMap)>,
    ) {
        self.type_manager.register_types(name, types);
    }

    pub fn get_digest(&self) -> &Digest {
        &self.digest
    }
}

#[cfg(test)]
mod tests {
    use crate::objects::*;
    use crate::values::Value;
    use crate::{Datastore, DatastoreError};
    use cowlang::{PrimitiveType, TypeDefinition};

    use blockchain_simulator::AccountId;

    use std::collections::{BTreeMap, HashMap};

    pub fn make_dummy_types(_app_name: String) -> BTreeMap<String, (u16, ObjectFieldMap)> {
        let mut result = BTreeMap::new();
        let mut field_map = BTreeMap::new();

        let field_type = TypeDefinition::make_map(
            TypeDefinition::Primitive(PrimitiveType::String),
            TypeDefinition::Primitive(PrimitiveType::String),
        );
        field_map.insert(String::from("map_field"), (0, field_type));

        result.insert(String::from("dummy_type"), (0, field_map));
        return result;
    }

    #[test]
    fn get_put() {
        let app = "mystr";
        let obj_uid = random_object_uid();
        let db = Datastore::default();
        db.register_application(app.to_string(), make_dummy_types(app.to_string()));
        let owner: AccountId = 100;

        let result = db.get(obj_uid);
        assert_eq!(result.is_ok(), false);
        assert_eq!(
            result.err().unwrap(),
            DatastoreError::NoSuchObject { uid: obj_uid }
        );

        let invalid_id = 0;

        let result = db.create(invalid_id, owner, &app, 0, HashMap::new());
        assert_eq!(result.is_ok(), false);
        assert_eq!(result.err().unwrap(), DatastoreError::InvalidKey);

        let mut values = HashMap::new();
        values.insert(String::from("map_field"), Value::make_map());

        let result = db.create(obj_uid, owner, &app, 0, values);

        assert_eq!(result.is_ok(), true);

        let result = db.get(obj_uid);
        assert_eq!(result.is_ok(), true);
    }

    #[test]
    fn list_append() {
        let obj_id = random_object_uid();
        let db = Datastore::default();
        let app = "myapp";
        let owner: AccountId = 100;

        db.register_application(app.to_string(), make_dummy_types(app.to_string()));

        let mut values = HashMap::new();
        values.insert(String::from("map_field"), Value::make_map());
        db.create(obj_id.clone(), owner, &app, 0, values).unwrap();

        let value1: Value = "foo bar cornell".into();

        let res1 = db.list_append(obj_id, &["wrong_field"], value1.clone());
        assert_eq!(res1, Err(DatastoreError::InvalidArguments));

        let res2 = db.list_append(obj_id, &["map_field"], value1.clone());
        assert_eq!(res2, Err(DatastoreError::TypeMismatch));

        let value2 = Value::make_list();
        let res3 = db.map_insert(obj_id, &["map_field", "mylist"], value2);
        assert_eq!(res3, Ok(()));

        let res4 = db.list_append(obj_id, &["map_field", "mylist"], value1.clone());
        assert_eq!(res4, Ok(()));

        let res5 = db.list_append(obj_id, &["map_field", "myotherlist"], value1);

        assert_eq!(res5, Ok(()));
    }
}
